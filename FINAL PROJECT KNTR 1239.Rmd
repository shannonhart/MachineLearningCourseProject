Generating an Accurate Prediction Model for the HAR Data Set
========================================================
### submitted by SHR - 9.21.2014

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 


For this analysis, our goal was to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

We will build a prediction model using the "training" data set to predict the "classe" variable in the test set.

Two data sets will be used in the analysis and can be found here:

training data https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

test data:  https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


### Data Exploration:
Install packages and load libraries

library(ggplot2)
library(lattice)
library(rpart)
library(caret)
library(randomForest)
library(ElemStatLearn)
install.packages("e1071")



#### The data is divided into 5 Classes (Classes A-E):

Class A corresponds to the specified execution of the exercise:

Perfoming task exactly according to the specification

Classes B - E correspond to common mistakes:

B: throwing the elbows to the front

C: lifting the dumbbell only halfway

D: lowering the dumbbell only halfway

E: throwing the hips to the front

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (See the section on the Weight Lifting Exercise Dataset)
```{r}
training <-read.csv("pml-training.csv", header=TRUE)
test <-read.csv("pml-testing.csv", header=TRUE)
```
I did a quick pass exploratory data structure analysis.

```{r}
dim(training)
dim(test)

table(training$classe)

```




### Preprocessing Data

I sorted through the data set column names using the "colnames(training)"" function to see if there were any extra columns that I could remove.
I recognized that I am able to remove the first 6 columns to better refine data set. I ran the "colnames" function on the test data and see that I am able to refine the test data the same way I refined the training data. 

```{r}
training <- training[, 6:dim(training)[2]]

test <- test[, 6:dim(test)[2]]
```

To further refine the data sets I removed the "NAs" from both sets.

```{r}
Cleantrainset<-training
Cleantrainset[ Cleantrainset== '' | Cleantrainset == 'NA'] <- NA
fixed <-which(colSums(is.na(Cleantrainset))!=0)
Cleantrainset<-Cleantrainset[, -fixed]

```
```{r}
cleantestset<-test
cleantestset[ cleantestset== '' | cleantestset == 'NA'] <- NA
fixed1 <-which(colSums(is.na(cleantestset))!=0)
cleantestset<-cleantestset[, -fixed1]
```

To get a visual representation of the possible relationships between class and activity, I have created 4 density plots - choosing 4 
activities/monitor results in various categories.

#### Figures 1-4

```{r}
library(ggplot2)
par(mfrow=c(2,2))
qplot(accel_dumbbell_y,colour=classe,data=Cleantrainset,geom="density")
 qplot(accel_forearm_y,colour=classe,data=Cleantrainset,geom="density")
 qplot(gyros_arm_z,colour=classe,data=Cleantrainset,geom="density")
qplot(magnet_arm_z ,colour=classe,data=Cleantrainset,geom="density")
```

We can use caret::train() to determine the optimal parameters for a model. This function will repeatedly resample the data set in order to estimate the effect of different parameters. When it is done, it will report the optimal parameters, an estimated accuracy, and an estimated standard deviation for the accuracy.


### Cross validation
I split the train data set into 2 sets to cross validate the model fit. I set the seed to (2322) for reproducibiity.
```{r}
set.seed(2322)
library(caret)
InTrain <- createDataPartition(y= Cleantrainset $classe,p=0.75,list=FALSE)
Cleantrainset <- Cleantrainset [InTrain,]
Subvaltrainset <- Cleantrainset [-InTrain,]
```
We can set tuneLength = 1 to denote the number of levels for each tuning parameters that should be generated by train.  We will run the Random Forest function to see if this model is a good prediction model for this data set.  

In this instance, this model seems like a great fit for predictions - citing a 100 % accuracy rate.

```{r} 
library(caret)
modelFit<- train(classe~., data=Cleantrainset, method = "rf", tuneLength = 1, ntree = 35)
print(modelFit)
```


Using cross validation to test the accuracy of the Random Forest prediction model, we see that it is indeed a great fit.
```{r}

predictions <-predict(modelFit, newdata=Subvaltrainset)
confusionMatrix(predictions,Subvaltrainset$classe)

```

### Predictions using the test data set



Run test data
```{r}
predictiontest <-predict(modelFit,cleantestset)
print(predictiontest)
class(predictiontest)

```
### Conclusion:
I got lucky using the Random Forest model first and seeing that it was an excellent fit for this data set.  I tried a "glm" model type as well and that was not successful.

Running code provided in submission section to generate file for answer submissions.

```(r)
setwd("C://Users//shannon//Desktop//MACHINE LEARNING//answers");
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
};
pml_write_files(predictiontest)
```
```{r}
setwd("C://Users//shannon//Desktop//MACHINE LEARNING")
```